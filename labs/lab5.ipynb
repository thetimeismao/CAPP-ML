{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Ensemble Methods: Bagging, Boosting, Random Forests\n",
    "\n",
    "In this lab you will get familiar with ensemble methods. We will cover bagging, boosting (AdaBoost) and random forests in the exercises in this lab. Please refer to \n",
    "http://scikit-learn.org/stable/modules/ensemble.html#adaboost for an introduction to these methods and some example usages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Import and cleaning\n",
    "We will use the same data as in the previous two labs (Kaggle KDD Cup 2014) so you can use the same cleaned data for\n",
    "this lab session. In order to save some time you can also use the following parts taken from the solution to the\n",
    "previous lab and modify as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "proj = pd.read_csv('data/projects.csv')\n",
    "outcomes = pd.read_csv('data/outcomes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join on project id\n",
    "all_data = pd.merge(proj, outcomes, how='inner', left_on='projectid', right_on='projectid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pick the columns you want to use\n",
    "# include 'is_exciting' column here'\n",
    "main_cols = [ 'is_exciting']  # add the other columns\n",
    "\n",
    "main_variables = all_data[main_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_bools(df):\n",
    "    d = {'t': True, 'f': False}\n",
    "    return df.replace(d)\n",
    "\n",
    "    \n",
    "# apply it to the dataframe\n",
    "main_variables = convert_bools(main_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize_categories(df, cat_cols, drop=True):\n",
    "    '''\n",
    "    df: a pandas dataframe\n",
    "    cat_cols: list of column names to generate indicator columns for\n",
    "    drop: a bool. If true, drop the original category columns\n",
    "    Returns: the modified dataframe\n",
    "    '''\n",
    "    for col in cat_cols:\n",
    "        binary_cols = pd.get_dummies(df[col], col)\n",
    "        df = pd.merge(df, binary_cols, left_index=True, right_index=True, how='inner')\n",
    "    if drop:\n",
    "        df.drop(cat_cols, inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# apply it to the real dataframe\n",
    "cat_cols = [  ]  # add the categorical columns\n",
    "cleaned_main_variables = binarize_categories(main_variables, cat_cols, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop rows with NaN\n",
    "main_variables = cleaned_main_variables.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate the features and the outcomes\n",
    "y_values = main_variables['is_exciting']\n",
    "del main_variables['is_exciting']\n",
    "x_values = main_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 80/20 train test split. But you can tweak the test size\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.20, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~ Empty DataFrame\n",
      "Columns: []\n",
      "Index: [514949, 208941, 327685, 52352, 207361, 235289, 372931, 558728, 598926, 304788, 197385, 533339, 595767, 448093, 204244, 551037, 151774, 462085, 69343, 48970, 52603, 334255, 437233, 123705, 111855, 591892, 596924, 93613, 511517, 492136, 424934, 295706, 235031, 87642, 239677, 31706, 433961, 568818, 188421, 423311, 118794, 247097, 59939, 60623, 615982, 70128, 356800, 342333, 122714, 373389, 307118, 15445, 87451, 228756, 216071, 300807, 220735, 161075, 522854, 342210, 290662, 105882, 137449, 435849, 540605, 600042, 43304, 498355, 364364, 63080, 52279, 83339, 309214, 143535, 552723, 383173, 461133, 116265, 95612, 24339, 183999, 269204, 491623, 282466, 295101, 452335, 467222, 259534, 565185, 112395, 222349, 556808, 12104, 453340, 116898, 429664, 62140, 32800, 260394, 240453, ...]\n",
      "\n",
      "[495460 rows x 0 columns] ~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~ Empty DataFrame\n",
      "Columns: []\n",
      "Index: [162593, 106968, 345944, 523604, 405064, 484512, 323436, 522388, 521416, 89782, 220396, 600217, 541722, 300487, 181771, 546768, 503813, 363066, 134464, 610773, 391537, 90639, 29823, 355382, 6023, 27577, 184717, 191502, 90311, 484613, 150347, 40512, 401936, 418138, 463271, 160177, 27221, 17033, 361612, 136148, 436339, 577829, 439187, 182071, 75908, 536669, 103863, 78119, 131760, 507384, 410246, 59217, 475068, 64218, 608845, 145721, 57659, 154348, 299691, 363428, 239419, 370508, 150038, 156555, 58488, 33887, 30347, 96381, 398358, 437702, 8881, 208778, 417390, 577521, 612252, 530745, 442723, 24126, 249698, 431955, 214225, 553855, 257080, 97343, 550586, 590347, 475299, 593041, 361442, 31293, 434112, 595052, 260924, 212196, 145174, 436301, 521348, 94069, 190204, 536876, ...]\n",
      "\n",
      "[123866 rows x 0 columns] ~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~ 514949    False\n",
      "208941    False\n",
      "327685    False\n",
      "52352     False\n",
      "207361    False\n",
      "235289    False\n",
      "372931    False\n",
      "558728    False\n",
      "598926    False\n",
      "304788    False\n",
      "197385    False\n",
      "533339    False\n",
      "595767    False\n",
      "448093    False\n",
      "204244    False\n",
      "551037    False\n",
      "151774    False\n",
      "462085    False\n",
      "69343      True\n",
      "48970     False\n",
      "52603      True\n",
      "334255    False\n",
      "437233    False\n",
      "123705    False\n",
      "111855    False\n",
      "591892    False\n",
      "596924    False\n",
      "93613     False\n",
      "511517    False\n",
      "492136    False\n",
      "          ...  \n",
      "220537    False\n",
      "56246      True\n",
      "398338    False\n",
      "508070    False\n",
      "125317    False\n",
      "559136    False\n",
      "315825    False\n",
      "434405    False\n",
      "3345      False\n",
      "321566     True\n",
      "86713     False\n",
      "400550    False\n",
      "573403    False\n",
      "122118    False\n",
      "450367    False\n",
      "578018    False\n",
      "543498    False\n",
      "54968     False\n",
      "261137    False\n",
      "1774      False\n",
      "510849    False\n",
      "79788      True\n",
      "299476    False\n",
      "358724    False\n",
      "287775    False\n",
      "516374    False\n",
      "125680    False\n",
      "491926    False\n",
      "297103    False\n",
      "589041    False\n",
      "Name: is_exciting, dtype: bool ~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~ 162593    False\n",
      "106968    False\n",
      "345944     True\n",
      "523604    False\n",
      "405064    False\n",
      "484512    False\n",
      "323436    False\n",
      "522388    False\n",
      "521416    False\n",
      "89782     False\n",
      "220396    False\n",
      "600217    False\n",
      "541722    False\n",
      "300487    False\n",
      "181771    False\n",
      "546768    False\n",
      "503813    False\n",
      "363066    False\n",
      "134464    False\n",
      "610773    False\n",
      "391537    False\n",
      "90639     False\n",
      "29823      True\n",
      "355382     True\n",
      "6023      False\n",
      "27577     False\n",
      "184717    False\n",
      "191502    False\n",
      "90311     False\n",
      "484613    False\n",
      "          ...  \n",
      "319563    False\n",
      "581928    False\n",
      "434383    False\n",
      "167308    False\n",
      "455526    False\n",
      "127220    False\n",
      "158883    False\n",
      "13798     False\n",
      "248125    False\n",
      "410004    False\n",
      "405826    False\n",
      "164482    False\n",
      "439911    False\n",
      "151928    False\n",
      "340029    False\n",
      "616371    False\n",
      "591809    False\n",
      "103240     True\n",
      "25822     False\n",
      "177444     True\n",
      "979        True\n",
      "246457    False\n",
      "179885    False\n",
      "528900    False\n",
      "320209    False\n",
      "281505    False\n",
      "286473     True\n",
      "252758     True\n",
      "395593    False\n",
      "382198    False\n",
      "Name: is_exciting, dtype: bool ~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "for x in [X_train, X_test, y_train, y_test]:\n",
    "    print('~'*18,\n",
    "          x,\n",
    "         '~'*18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Bagging\n",
    "You can use bagging to make a stronger estimator from simple base estimators. Bagging merges independent estimators which are made using different random subsets of the training samples. Refer to \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#r154 \n",
    "\n",
    "to check the parameters. This function provides more than just bagging, for instance in addition to you can also take random subsets of the features (max_features). In order to implement bagging, you need to keep all the features but use random subsets of the samples. You can use the bootstrap parameter to specify that your samples are drawn with replacement. Use n_estimators and max_samples to specify the number of the estimators you want to use and the number of samples you want to use for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bagging with Decision Tree Classifier\n",
    "Use Decision Tree as  your base classifier. You can start with depth 20 for your decision trees. Since the data\n",
    "is very unbalanced regarding to the number of True and False samples, use the class_weight parameter to specify\n",
    "how much the model should prefer correctly classifying one class over another.\n",
    "\n",
    "Define your classifier. Use fit and score functions to fit your model and compute the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###What happens when you try different bagging parameters?\n",
    "Try n_estimator = { 5 ,10, 20 } , max_depth = {10, 20} and max_samples = { 0.35, 0.5, 0.65 }\n",
    "and report the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Build Classifier: For this assignment, select any classifier you feel \n",
    "# comfortable with (Logistic Regression for example)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def classify(df, features, label, cl_method):\n",
    "    '''\n",
    "    Given training and testing data for independent variables (features),\n",
    "    training data for dependent variable, and classifying method,\n",
    "    return model, X_test, y_test\n",
    "    '''\n",
    "    if cl_method == \"KNN\":\n",
    "        model = KNeighborsClassifier(n_neighbors=13, \n",
    "                                     metric='minkowski', \n",
    "                                     weights='distance')\n",
    "    elif cl_method == \"Tree\":\n",
    "        model = tree.DecisionTreeClassifier()\n",
    "    elif cl_method == \"GB\":\n",
    "        model = GradientBoostingClassifier()\n",
    "    elif cl_method == \"Logit\":\n",
    "        model = LogisticRegression()\n",
    "    else:\n",
    "        raise ValueError('{} not currently avaliable'.format(cl_method))\n",
    "        \n",
    "    Y = df[label].head()\n",
    "    X = df[features].head()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        Y, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=2)\n",
    "    \n",
    "    model = model.fit(X_train, y_train)\n",
    "\n",
    "    return model, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. Evaluate Classifier: you can use any metric you choose for this assignment \n",
    "# (accuracy is the easiest one). Feel free to evaluate it on the same data you \n",
    "# built the model on (this is not a good idea in general but for this assignment, \n",
    "# it is fine). We haven't covered models and evaluation yet, so don't worry about \n",
    "# creating validation sets or cross-validation. \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
    "\n",
    "# credits to https://github.com/yhat/DataGotham2013/blob/master/notebooks/8%20-%20Fitting%20and%20Evaluating%20Your%20Model.ipynb\n",
    "\n",
    "def evaluate(model, X_te, y_te):\n",
    "    '''\n",
    "    Given the model and independent and dependent testing data,\n",
    "    print out statements that evaluate classifier\n",
    "    '''\n",
    "    probs = model.predict_proba(X_te)\n",
    "    \n",
    "    plt.hist(probs[:,1])\n",
    "    plt.xlabel('Likelihood of Significant Financial')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # We should also look at Accuracy\n",
    "    print(\"Accuracy = \" + str(model.score(X_te, y_te)))\n",
    "\n",
    "    # Finally -- Precision & Recall\n",
    "    y_hat = model.predict(X_te)\n",
    "    print(classification_report(y_te, y_hat, labels=[0, 1]))\n",
    "    \n",
    "    y_hat = model.predict(X_te)    \n",
    "    confusion_matrix = pd.crosstab(y_hat, \n",
    "                                   y_te, \n",
    "                                   rownames=[\"Actual\"], \n",
    "                                   colnames=[\"Predicted\"])\n",
    "    print(confusion_matrix)\n",
    "\n",
    "def plot_roc(probs, y_te):\n",
    "    '''\n",
    "    Plots ROC curve.\n",
    "    '''\n",
    "    plt.figure()\n",
    "    fpr, tpr, thresholds = roc_curve(y_te, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    pl.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    pl.plot([0, 1], [0, 1], 'k--')\n",
    "    pl.xlim([0.0, 1.05])\n",
    "    pl.ylim([0.0, 1.05])\n",
    "    pl.xlabel('False Positive Rate')\n",
    "    pl.ylabel('True Positive Rate')\n",
    "    pl.title(\"ROC Curve\")\n",
    "    pl.legend(loc=\"lower right\")\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bagging with logistic regression\n",
    "Now you will try using another base classifier. Use logistic regression as  your base classifier. To keep it simple use l2 norm and C = 1. Since the data is very unbalanced regarding to the number of True and False samples, use the class_weight parameter to specify how much the model should prefer correctly classifying one class over another.\n",
    "\n",
    "Define your classifier. Use fit and score functions to fit your model and compute the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###What happens when you try different bagging parameters?\n",
    "Try n_estimator = { 5 ,10, 20 } and max_samples = { 0.35, 0.5, 0.65 } and report the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Boosting - AdaBoost\n",
    "\n",
    "Another approach for making stronger estimators from the basic ones is boosting. In contrast to bagging, boosting makes a strong classifier by adding the features one by ones based on the predictive power. At each step of boosting training samples are re-weighted to give a higher weight to the ones which were wrongly classified and direct the algorithm to choose features which are useful for classifying those samples.\n",
    "\n",
    "In this part you will make a classifier using AdaBoost which as a popular boosting algorithm. Refer to\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier\n",
    "\n",
    "for the specifications. The default estimator is decision tree classifier but you can use any estimator of your choice as far as it has the conditions of the base estimator for AdaBoost. For instance since AdaBoost changes the weights of the samples, your base classifier should support this propoerty. Here again, you need to specify the number of simple classifiers by n_estimators.\n",
    "\n",
    "#### Make sure you have enough features\n",
    "\n",
    "For using AdaBoost since the base classifiers added at each step are made by taking new features, you need to make sure that you have enough variables to make the simple classifiers. So, if you took very few variables from your data to begin with, you will need to add other features for this part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost with decision tree of depth one\n",
    "Use decision tree of depth one as your base classifier. For Adaboost parameters use n_estimators = 5.  Again use the class_weight parameter for your decision tree classifier to deal with the unbalanced data. You may use the 'balanced' option.\n",
    "\n",
    "Define your classifier. Use fit and score functions to fit your model and compute the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when you decrease or increase the number of your estimators?\n",
    "\n",
    "Try using n_estimators = { 1, 2, 5, 10} and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Random Forests\n",
    "\n",
    "Another approach for making stronger estimators from the basic ones is using random forests which provides a strong etimator built from a number of decision tree estimators. Each individual decision tree is made by using a random subset of the features. In addidion to that the training samples used for each tree are also random bootstrap samples from the training set (of the same size).\n",
    "\n",
    "In this part you will make a random forest classifier. Refer to\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "for the specifications and the parameters. You can use n_estimators and max_features to specify the number of the estimators and the number of features you want to use for building each tree. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exeriment with Random Forest \n",
    "\n",
    "Use n_estimator = 10 and max_features=sqrt(n_features). You may use max_depth=20 in combination with min_samples_split=1 to stop the trees from growing too deep. Again use the class_weight parameter for your decision tree classifier to deal with the unbalanced data.\n",
    "\n",
    "Define your classifier. Use fit and score functions to fit your model and compute the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when you use other parameters?\n",
    "Use different values for n_estimator and max_depth. Report and compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
